{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Challenge\n",
    "\n",
    "1. Get the book \"Los Miserables\" by Victor Hugo, from Carlos Slim Foundation. \n",
    "   - https://aprende.org/pruebasat?sectionId=6\n",
    "2. Convert the book into text (CSV)\n",
    "3. Clean the csv file:\n",
    "   - Standarize (no upper chars, no whitespace, no punctuation, accents, if possible).\n",
    "   - Create a vocabulary with the words in the book, following the ideas when discussed how to create a vocabulary in the notes of this slide set.\n",
    "   - Store the vocabulary in parquet format.\n",
    "   - Do statistics, including:\n",
    "     + How many words in book.\n",
    "     + How many different words in vocabulary.\n",
    "     + Print the 100 most frequent words.\n",
    "4. Produce a 2 page report to describe your experience, methods, etc.\n",
    "5. Write code in a language of your choosing from this set (Go, Julia, Python)\n",
    "6. Submit by 14/03/2025 @ 16:00 UTC-6 (Mexico City Time), inside the \"python/src/student_submissions/Vocabulary\" folder.\n",
    "7. You must create a folder named \"lastname_firstname\" to put your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from unicodedata import normalize\n",
    "from copy import copy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeramente, definimos las rutas de los archivos y abrimos el archivo pdf.\n",
    "Con la ayuda de la librería PyPDF convertimos el archivo a texto plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número de páginas es 305\n"
     ]
    }
   ],
   "source": [
    "path = \"/mnt/c/Users/omarm/Downloads/\"\n",
    "name = \"Los-miserables\"\n",
    "\n",
    "reader = PdfReader(path+name+'.pdf')\n",
    "number_of_pages = len(reader.pages)\n",
    "\n",
    "print(f\"El número de páginas es {number_of_pages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El total de caracteres en el libro es: 640742\n"
     ]
    }
   ],
   "source": [
    "all_text = ''\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    all_text+=text+'\\n'\n",
    "\n",
    "print(f\"El total de caracteres en el libro es: {len(all_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza del texto\n",
    "\n",
    "Creamos una función que nos ayudara a limpiar el texto extraído, convirtiendo todo a minúsculas, quitando diacríticos excepto la __ñ__, eliminando números y signos de puntuación, de esta manera logramos un texto plano limpio y que nos permitirá trabajarlo de manera más adecuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text_f = copy(text)\n",
    "    text_f = text_f.lower()\n",
    "    # -> NFD y eliminar diacríticos\n",
    "    text_f = re.sub(\n",
    "        r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+\", r\"\\1\",\n",
    "        normalize(\"NFD\", text_f), 0, re.I)\n",
    "    text_f = normalize('NFC', text_f)\n",
    "    text_f = re.sub(\"\\d+\", \"\", text_f)\n",
    "    text_f = re.sub(r'[^\\w0-9()\\s]', '', text_f)\n",
    "\n",
    "    return text_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente es una muestra de cómo se ve el texto resultante ya limpiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " senador del imperio antiguo miembro del consejo de los quinientos favorable al  brumario y agraciado cerca de la ciudad de digne con una magnifica senaduria escribio al ministro de cultos bigot de preameneu una nota irritada y confidencial de la cual extraemos estas lineas autenticas gastos de car\n"
     ]
    }
   ],
   "source": [
    "final_text = clean_text(all_text)\n",
    "print(final_text[12101:12400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar el vocabulario en formato CSV\n",
    "\n",
    "Primeramente creamos el vocabulario del libro contando el número de veces que se repite cada palabra y guardándolo en un diccionario, para posteriormente convertirlo a un dataframe y finalmente guardarlo en formato CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Counter(final_text.split())\n",
    "#dict(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {'word': list(vocabulary.keys()), 'frequency': list(vocabulary.values())}\n",
    "vocabulary_df = pd.DataFrame.from_dict(data)\n",
    "vocabulary_df.to_csv(path+name+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardar el archvo en formato parquet\n",
    "\n",
    "Guardamos el archivo ayudándonos de la librería pyarrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_df.to_parquet(path+name+'.parquet', index=False, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para mejorar el formato de impresion de las palabras.\n",
    "def format_codes(codes: list[str], limit: int = 100) -> str:\n",
    "    \"\"\"Remove the word _copy and return a string with all codes.\"\"\"\n",
    "    short_codes = codes if len(codes) < limit else list(codes)[0:limit]\n",
    "    msg = \", \".join([c for c in short_codes])\n",
    "    if len(codes) > limit:\n",
    "        msg += f\" y otros {len(list(codes)[limit:])}.\"\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estadística del libro\n",
    "\n",
    "Finalmente obtenemos algunos datos de relevancia acerca del libro, primeramente, obtuvimos que el total de palabras que contiene es de __109271__, siendo un total de __13125__ palabras distintas las que se utilizan. Se muestran las 100 palabras más comunes y las 100 menos comunes con su respectiva frecuencia, finalmente también se obtiene el valor medio de uso de una palabra en el libro que es de __8.32__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palabras en el libro: 109271\n",
      "Cantidad de palabras distintas en el libro: 13135\n",
      "Las 100 palabras mas comunes son: \"de\" con 5325, \"la\" con 3918, \"que\" con 3818, \"el\" con 3393, \"y\" con 3123, \"en\" con 2836, \"a\" con 2488, \"se\" con 1681, \"un\" con 1601, \"no\" con 1498, \"los\" con 1352, \"una\" con 1316, \"su\" con 1245, \"las\" con 935, \"por\" con 935, \"con\" con 924, \"habia\" con 858, \"del\" con 813, \"al\" con 756, \"es\" con 749, \"lo\" con 719, \"le\" con 667, \"era\" con 650, \"como\" con 571, \"mas\" con 513, \"para\" con 504, \"señor\" con 447, \"esta\" con 414, \"pero\" con 372, \"hombre\" con 363, \"si\" con 358, \"sus\" con 344, \"todo\" con 327, \"me\" con 326, \"sin\" con 311, \"obispo\" con 286, \"dijo\" con 281, \"cuando\" con 274, \"estaba\" con 273, \"sobre\" con 269, \"dos\" con 264, \"este\" con 261, \"aquel\" con 253, \"mi\" con 244, \"ya\" con 229, \"hacia\" con 219, \"esto\" con 218, \"yo\" con 218, \"madeleine\" con 214, \"tenia\" con 211, \"ha\" con 199, \"jean\" con 199, \"fantine\" con 194, \"valjean\" con 192, \"aquella\" con 190, \"hay\" con 186, \"he\" con 181, \"ser\" con 181, \"muy\" con 178, \"javert\" con 175, \"nada\" con 174, \"mismo\" con 173, \"o\" con 164, \"os\" con 163, \"poco\" con 158, \"tan\" con 158, \"bien\" con 157, \"ni\" con 156, \"ella\" con 155, \"quien\" con 151, \"alcalde\" con 149, \"vez\" con 148, \"despues\" con 146, \"fue\" con 145, \"todos\" con 141, \"puerta\" con 137, \"años\" con 136, \"hubiera\" con 133, \"cual\" con 133, \"donde\" con 131, \"dios\" con 130, \"mujer\" con 127, \"momento\" con 125, \"sido\" con 124, \"tiempo\" con 124, \"casa\" con 123, \"son\" con 121, \"aqui\" con 119, \"noche\" con 119, \"hecho\" con 118, \"tres\" con 115, \"dia\" con 114, \"luego\" con 113, \"cabeza\" con 113, \"decir\" con 112, \"voz\" con 111, \"alli\" con 107, \"ojos\" con 107, \"monseñor\" con 105, \"aun\" con 105\n",
      "Las 100 palabras menos comunes son: \"inarticulados\" con 1, \"persiguiendome\" con 1, \"turbaria\" con 1, \"alboroto\" con 1, \"murmullos\" con 1, \"protestas\" con 1, \"ambiente\" con 1, \"respirable\" con 1, \"tranquilos\" con 1, \"alocado\" con 1, \"salvarse\" con 1, \"devolverle\" con 1, \"robados\" con 1, \"proyectos\" con 1, \"abominables\" con 1, \"desfallecer\" con 1, \"importunan\" con 1, \"indistintamente\" con 1, \"ataque\" con 1, \"cedia\" con 1, \"acumulado\" con 1, \"entablado\" con 1, \"preguntarse\" con 1, \"terriblemente\" con 1, \"atisbo\" con 1, \"hallara\" con 1, \"explicando\" con 1, \"ocurrian\" con 1, \"ebrio\" con 1, \"extraviados\" con 1, \"zumbidos\" con 1, \"advierten\" con 1, \"guardaria\" con 1, \"maestra\" con 1, \"lateral\" con 1, \"registrado\" con 1, \"puestos\" con 1, \"velaban\" con 1, \"colgo\" con 1, \"esperara\" con 1, \"reservar\" con 1, \"circulado\" con 1, \"exactos\" con 1, \"envolvio\" con 1, \"boquiabierta\" con 1, \"retenido\" con 1, \"embargada\" con 1, \"barrote\" con 1, \"integramente\" con 1, \"objecion\" con 1, \"correcto\" con 1, \"amuralladas\" con 1, \"retirarse\" con 1, \"quedarse\" con 1, \"aventurar\" con 1, \"dominio\" con 1, \"devuelta\" con 1, \"evadido\" con 1, \"mintio\" con 1, \"seguidas\" con 1, \"holocausto\" con 1, \"valga\" con 1, \"reparo\" con 1, \"singularidad\" con 1, \"blanc\" con 1, \"enseñara\" con 1, \"partidarios\" con 1, \"despavorida\" con 1, \"reflexionando\" con 1, \"boujean\" con 1, \"almibarado\" con 1, \"rehusado\" con 1, \"concededme\" con 1, \"acompañareis\" con 1, \"bromear\" con 1, \"crei\" con 1, \"fueras\" con 1, \"pides\" con 1, \"abonada\" con 1, \"drapeau\" con 1, \"respondedme\" con 1, \"callate\" con 1, \"cuidadas\" con 1, \"condesas\" con 1, \"asia\" con 1, \"firmemente\" con 1, \"hables\" con 1, \"escuchare\" con 1, \"huella\" con 1, \"penultima\" con 1, \"obtuvo\" con 1, \"interesa\" con 1, \"reservandole\" con 1, \"acabemos\" con 1, \"irte\" con 1, \"dices\" con 1, \"discursos\" con 1, \"ahorremonos\" con 1, \"vamonos\" con 1, \"usare\" con 1\n",
      "La frecuencia media de una palabra en el libro es de 8.319071183859917\n"
     ]
    }
   ],
   "source": [
    "vocabulary_df = pd.read_parquet(path+name+'.parquet')\n",
    "\n",
    "print(f\"Total de palabras en el libro: {vocabulary_df['frequency'].sum()}\")\n",
    "print(f\"Cantidad de palabras distintas en el libro: {len(vocabulary_df)}\")\n",
    "\n",
    "most_com = vocabulary_df.sort_values(by='frequency', ascending=False).head(100)\n",
    "less_com = vocabulary_df.sort_values(by='frequency', ascending=True).head(100)\n",
    "mean = vocabulary_df['frequency'].mean()\n",
    "\n",
    "print(f\"Las 100 palabras mas comunes son: {format_codes([f'\"{row['word']}\" con {int(row.frequency)}' for row in most_com.iloc])}\")\n",
    "print(f\"Las 100 palabras menos comunes son: {format_codes([f'\"{row['word']}\" con {int(row.frequency)}' for row in less_com.iloc])}\")\n",
    "\n",
    "print(f\"La frecuencia media de una palabra en el libro es de {mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diplomado-ia-Zsy5ah72-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
