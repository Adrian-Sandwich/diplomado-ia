{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Challenge**\n",
        "\n",
        "1. Get the the notebook from https://keras.io/examples/nlp/neural_machine_translation_with_transformer/\n",
        "2. Make it run. Note that it uses the spa-eng file from the Anki site\n",
        "3. Include code to save the model on disk so that you can use the pre-trained model\n",
        "4. Include code to use the pre-trained embeddings from Stanford.\n",
        "Link at https://nlp.stanford.edu/projects/glove/\n",
        "5. Include code to show the layer activations as the ones shown the notebook that shown during the lecture.\n",
        "Code at https://github.com/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb\n",
        "6. Work with the model to improve its performance. Things to try are:\n",
        "* Use more than 30 epochs\n",
        "* Change the number of ngrams\n",
        "* Change the learning rate\n",
        "* Change the optimizer\n",
        "* Change the metric\n",
        "* Explore how to use the BLUE (Bilingual Evaluation Understudy)\n",
        "* Explore how to use the Rouge  score\n",
        "\n",
        "OPTIONAL: Get and run the code from https://keras.io/examples/nlp/neural_machine_translation_with_keras_hub/\n",
        "to use the Rouge metric\n",
        "\n",
        "Write a short report (5 pages) describing your work, results, comments\n",
        "Deadline: 03/22/2025 @ Noon, CDMX Time, using the Github page :\n",
        "https://github.com/camachojua/diplomado-ia/tree/main/python/src/student_submissions/Transformer"
      ],
      "metadata": {
        "id": "WXHKZ9LG3xLs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dGP99PZweQ-z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.data as tf_data\n",
        "import tensorflow.strings as tf_strings\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import ops\n",
        "from keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng_extracted\" / \"spa-eng\" / \"spa.txt\""
      ],
      "metadata": {
        "id": "bT7DgNfiebvK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eea862d0-dea7-44d5-e99b-ca986560d5f9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "\u001b[1m2638744/2638744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analizando los datos\n",
        "Cada línea contiene una oración en inglés y su correspondiente oración en español. La oración en inglés es la secuencia de origen y la oración en español es la secuencia objetivo. Anteponemos el token \"[start]\" y añadimos el token \"[end]\" a la oración en español.\n",
        "\n",
        "with open(text_file) as f:"
      ],
      "metadata": {
        "id": "fQbOG_YAfTYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    text_pairs.append((eng, spa))"
      ],
      "metadata": {
        "id": "6iEaRZdwfSwU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xX--Awe_faDr",
        "outputId": "9e1266c5-5689-4987-e735-92446d2bd503"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Can I stay here tonight?', '[start] ¿Puedo pasar aquí la noche? [end]')\n",
            "(\"I'm shorter than him.\", '[start] Soy más pequeño que él. [end]')\n",
            "(\"I don't like the way she laughs.\", '[start] No me gusta la manera en la que ella ríe. [end]')\n",
            "('I wish I were a good singer.', '[start] Quisiera ser un buen cantante. [end]')\n",
            "('Tom has lied to you.', '[start] Tom te ha mentido. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "46Q9be7mfahQ",
        "outputId": "2e6c8e8f-66d4-40ba-e779-17dbf3473c3a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964 total pairs\n",
            "83276 training pairs\n",
            "17844 validation pairs\n",
            "17844 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vectorizando los datos de texto"
      ],
      "metadata": {
        "id": "j8xuU1yQfkUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf_strings.lower(input_string)\n",
        "    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        "    ngrams=4,####<-----n-grams\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_spa_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "spa_vectorization.adapt(train_spa_texts)"
      ],
      "metadata": {
        "id": "9QQwzyIJfks8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return (\n",
        "        {\n",
        "            \"encoder_inputs\": eng,\n",
        "            \"decoder_inputs\": spa[:, :-1],\n",
        "        },\n",
        "        spa[:, 1:],\n",
        "    )\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.cache().shuffle(2048).prefetch(16)\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "lbRVSQ2lfohk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OFCjw1j-fr8R",
        "outputId": "35a2cad6-11e3-43ae-834f-e33a7d04d083"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
            "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Construyendo el modelo"
      ],
      "metadata": {
        "id": "D0OOyqTOfwFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.ops as ops\n",
        "\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(dense_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = ops.cast(mask[:, None, :], dtype=\"int32\")\n",
        "        else:\n",
        "            padding_mask = None\n",
        "\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"dense_dim\": self.dense_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = ops.shape(inputs)[-1]\n",
        "        positions = ops.arange(0, length, 1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return ops.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"sequence_length\": self.sequence_length,\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        inputs, encoder_outputs = inputs\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        if mask is None:\n",
        "            inputs_padding_mask, encoder_outputs_padding_mask = None, None\n",
        "        else:\n",
        "            inputs_padding_mask, encoder_outputs_padding_mask = mask\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask,\n",
        "            query_mask=inputs_padding_mask,\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            query_mask=inputs_padding_mask,\n",
        "            key_mask=encoder_outputs_padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = ops.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = ops.arange(sequence_length)[:, None]\n",
        "        j = ops.arange(sequence_length)\n",
        "        mask = ops.cast(i >= j, dtype=\"int32\")\n",
        "        mask = ops.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = ops.concatenate(\n",
        "            [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])],\n",
        "            axis=0,\n",
        "        )\n",
        "        return ops.tile(mask, mult)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"latent_dim\": self.latent_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "zjSuqG3OfuFS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)([x, encoder_outputs])\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "transformer = keras.Model(\n",
        "    {\"encoder_inputs\": encoder_inputs, \"decoder_inputs\": decoder_inputs},\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")"
      ],
      "metadata": {
        "id": "hRAR3sbyf0ff"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrenando el modelo"
      ],
      "metadata": {
        "id": "_vtnRDt1f5di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "epochs = 6  # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "custom_optimizer = Adam(learning_rate=0.001)####<------Tasa de aprendizaje para ADAM\n",
        "transformer.compile(\n",
        "    custom_optimizer,\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=0),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "LNAqmAbEf3L1",
        "outputId": "2a5e5c91-4a73-40bb-f289-cad149211f16"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m3,845,120\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)     │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (\u001b[38;5;33mNotEqual\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m3,845,120\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)     │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m3,155,456\u001b[0m │ positional_embedding[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_1 (\u001b[38;5;33mNotEqual\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_decoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │      \u001b[38;5;34m5,259,520\u001b[0m │ positional_embedding_… │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)      │                        │                │ transformer_encoder[\u001b[38;5;34m0\u001b[0m… │\n",
              "│                           │                        │                │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │                        │                │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ transformer_decoder[\u001b[38;5;34m0\u001b[0m… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15000\u001b[0m)    │      \u001b[38;5;34m3,855,000\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)     │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,845,120</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)     │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,155,456</span> │ positional_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_decoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,259,520</span> │ positional_embedding_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)      │                        │                │ transformer_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│                           │                        │                │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │                        │                │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,855,000</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,960,216\u001b[0m (76.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,960,216</span> (76.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 69ms/step - accuracy: 0.0776 - loss: 5.4451 - val_accuracy: 0.1291 - val_loss: 3.7411\n",
            "Epoch 2/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 62ms/step - accuracy: 0.1384 - loss: 3.6498 - val_accuracy: 0.1715 - val_loss: 2.8906\n",
            "Epoch 3/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 61ms/step - accuracy: 0.1775 - loss: 2.8296 - val_accuracy: 0.1980 - val_loss: 2.4332\n",
            "Epoch 4/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 62ms/step - accuracy: 0.2031 - loss: 2.3007 - val_accuracy: 0.2096 - val_loss: 2.1870\n",
            "Epoch 5/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 62ms/step - accuracy: 0.2200 - loss: 1.9647 - val_accuracy: 0.2187 - val_loss: 2.0458\n",
            "Epoch 6/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 63ms/step - accuracy: 0.2327 - loss: 1.7102 - val_accuracy: 0.2200 - val_loss: 1.9942\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f8b823e9ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Modificaciones a la clase `PositionalEmbedding` para cargar los embeddings de GloVe, se crea la clase `PositionalEmbeddingGlove`\n",
        "\n",
        "## Descripción\n",
        "Se realizaron modificaciones a la clase `PositionalEmbedding` para que no solo maneje las representaciones posicionales, sino también para cargar embeddings preentrenados de GloVe. Esto mejora la calidad de las representaciones de las palabras al aprovechar los vectores de palabras preentrenados de un modelo de embeddings de palabras como GloVe.\n",
        "\n",
        "## Modificaciones\n",
        "1. **Carga de los embeddings de GloVe**:\n",
        "   - Los embeddings de GloVe fueron cargados desde un archivo de texto que contiene los vectores preentrenados.\n",
        "   - Estos vectores fueron almacenados en un diccionario, donde las claves son las palabras y los valores son sus representaciones vectoriales.\n",
        "\n",
        "2. **Integración de los embeddings de GloVe en la capa de embedding**:\n",
        "   - En lugar de usar un embedding aleatorio, se cargó una matriz de embeddings preentrenados y se asignó a la capa de embeddings de la clase `PositionalEmbedding`.\n",
        "   - Solo las palabras que estén en el vocabulario de entrada serán representadas con los embeddings de GloVe. Las palabras fuera del vocabulario son representadas con un vector de ceros o con un valor de inicialización predeterminado.\n"
      ],
      "metadata": {
        "id": "FvfTXNWN35Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbeddingGlove(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, embedding_matrix=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embed_dim,\n",
        "            weights=[embedding_matrix] if embedding_matrix is not None else None,\n",
        "            trainable=False if embedding_matrix is not None else True\n",
        "        )\n",
        "\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = ops.shape(inputs)[-1]\n",
        "        positions = ops.arange(0, length, 1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return ops.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"sequence_length\": self.sequence_length,\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "VNU4j_It4Bx3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar y usar los embeddings preentrenados de GloVe\n",
        "\n",
        "## Descripción\n",
        "En esta celda se cargan los embeddings preentrenados de GloVe y se integran en la red neuronal. Los embeddings de GloVe son utilizados para representar las palabras en el vocabulario antes de pasar por el modelo Transformer.\n",
        "\n",
        "## Pasos realizados\n",
        "\n",
        "1. **Carga de los embeddings de GloVe**:\n",
        "   - Se carga el archivo de GloVe (`glove.6B.50d.txt`) que contiene vectores preentrenados de palabras en un diccionario llamado `embeddings_index`. Cada palabra se mapea a su correspondiente vector de características.\n",
        "\n",
        "2. **Creación de la matriz de embeddings**:\n",
        "   - Se construye una matriz llamada `embedding_matrix`, la cual tiene un tamaño igual al del vocabulario de entrada. Cada palabra en el vocabulario se mapea al vector de GloVe correspondiente si está presente en el diccionario `embeddings_index`.\n",
        "\n",
        "3. **Modificación del modelo**:\n",
        "   - Se crea una capa personalizada llamada `PositionalEmbeddingGlove`, la cual recibe la matriz de embeddings de GloVe para ser utilizada en el modelo.\n",
        "   - El encoder y decoder del modelo Transformer ahora utilizan esta capa de embeddings para representar las palabras de manera más informada."
      ],
      "metadata": {
        "id": "COUeE40T4J8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta al archivo GloVe\n",
        "glove_path = \"glove.6B.50d.txt\"\n",
        "\n",
        "# Cargar GloVe en un diccionario\n",
        "embeddings_index = {}\n",
        "with open(glove_path, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]  # Primera palabra en la línea\n",
        "        coefs = np.asarray(values[1:], dtype=\"float32\")  # Resto son los valores del embedding\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Se cargaron {len(embeddings_index)} palabras preentrenadas.\")\n",
        "\n",
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "vocab_size = len(spa_vocab)  # Número de palabras en el vocabulario\n",
        "embedding_dim = 50  # Tamaño del embedding de GloVe\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for i, word in enumerate(spa_vocab):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector  # Usar embedding preentrenado\n",
        "\n",
        "\n",
        "embed_dim = 50 # Cambiamos al tamaño del embedding de GloVe para armar el modelo\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "# Crea la capa PositionalEmbedding con la matriz de embeddings preentrenados\n",
        "embedding_layer = PositionalEmbeddingGlove(\n",
        "    sequence_length, vocab_size, embed_dim, embedding_matrix=embedding_matrix\n",
        ")\n",
        "\n",
        "# Construcción del encoder\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = embedding_layer(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "# Construcción del decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = embedding_layer(decoder_inputs)  # Usar la misma capa de embedding\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)([x, encoder_outputs])\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "# Crear el modelo completo\n",
        "transformer2 = keras.Model(\n",
        "    {\"encoder_inputs\": encoder_inputs, \"decoder_inputs\": decoder_inputs},\n",
        "    decoder_outputs,\n",
        "    name=\"transformer\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lKyLYurV4JsV",
        "outputId": "bf99a70e-57ec-4c7c-8475-4785f389f0bd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se cargaron 130220 palabras preentrenadas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto podemos compilar el modelo llamado transformer2 en el cual la capa de positional embedding usa los datos preentrenados de https://nlp.stanford.edu/projects/glove/"
      ],
      "metadata": {
        "id": "Bs91ZBUF4Z9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 6\n",
        "\n",
        "transformer2.summary()\n",
        "transformer2.compile(\n",
        "    \"rmsprop\",\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(ignore_class=0),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "transformer2.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "xxIRpx1L4PXX",
        "outputId": "b683810e-4bf3-4526-dc6d-5e14f1b019ad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"transformer\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ decoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_inputs            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding_glo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)       │        \u001b[38;5;34m751,000\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mPositionalEmbeddingGlov…\u001b[0m │                        │                │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_2 (\u001b[38;5;33mNotEqual\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)       │        \u001b[38;5;34m288,348\u001b[0m │ positional_embedding_… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_3 (\u001b[38;5;33mNotEqual\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_decoder_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)       │        \u001b[38;5;34m369,698\u001b[0m │ positional_embedding_… │\n",
              "│ (\u001b[38;5;33mTransformerDecoder\u001b[0m)      │                        │                │ transformer_encoder_1… │\n",
              "│                           │                        │                │ not_equal_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │                        │                │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)       │              \u001b[38;5;34m0\u001b[0m │ transformer_decoder_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15000\u001b[0m)    │        \u001b[38;5;34m765,000\u001b[0m │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ decoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ encoder_inputs            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ positional_embedding_glo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">751,000</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddingGlov…</span> │                        │                │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">288,348</span> │ positional_embedding_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ not_equal_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_decoder_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">369,698</span> │ positional_embedding_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecoder</span>)      │                        │                │ transformer_encoder_1… │\n",
              "│                           │                        │                │ not_equal_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │                        │                │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)       │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_decoder_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">765,000</span> │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,174,046\u001b[0m (8.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,174,046</span> (8.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,424,046\u001b[0m (5.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,424,046</span> (5.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m750,000\u001b[0m (2.86 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">750,000</span> (2.86 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 29ms/step - accuracy: 0.0596 - loss: 6.4812 - val_accuracy: 0.0950 - val_loss: 4.8059\n",
            "Epoch 2/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - accuracy: 0.1015 - loss: 4.9523 - val_accuracy: 0.1254 - val_loss: 4.2418\n",
            "Epoch 3/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 18ms/step - accuracy: 0.1198 - loss: 4.5819 - val_accuracy: 0.1406 - val_loss: 4.0138\n",
            "Epoch 4/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - accuracy: 0.1310 - loss: 4.3914 - val_accuracy: 0.1464 - val_loss: 3.8773\n",
            "Epoch 5/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 19ms/step - accuracy: 0.1378 - loss: 4.2646 - val_accuracy: 0.1528 - val_loss: 3.7953\n",
            "Epoch 6/6\n",
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 18ms/step - accuracy: 0.1419 - loss: 4.1972 - val_accuracy: 0.1569 - val_loss: 3.7511\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f8b24102e10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install nltk rouge-score"
      ],
      "metadata": {
        "id": "Dyu5bq5Axien"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Métricas BLEU y ROUGE\n",
        "\n",
        "Las métricas BLEU y ROUGE son utilizadas para evaluar la calidad de los sistemas de traducción automática, comparando las traducciones generadas por el modelo con las traducciones de referencia.\n",
        "\n",
        "### BLEU (Bilingual Evaluation Understudy)\n",
        "- **BLEU** mide la precisión n-grama de una traducción, comparando la cantidad de n-gramas coincidentes entre la traducción generada y las referencias.\n",
        "- Se utiliza un **suavizado** para evitar ceros en los cálculos de n-gramas raros o ausentes.\n",
        "- El cálculo de BLEU se realiza usando la función `corpus_bleu` de la librería `nltk`.\n",
        "\n",
        "### ROUGE (Recall-Oriented Understudy for Gisting Evaluation)\n",
        "- **ROUGE** evalúa la calidad de la traducción basándose en la comparación de n-gramas y subsecuencias con las referencias.\n",
        "- Se considera principalmente el **recall** y la **longitud L** (ROUGE-L), que mide la longitud máxima de la subsecuencia común más larga.\n",
        "- Se calcula usando la librería `rouge_score`.\n"
      ],
      "metadata": {
        "id": "Eag54m8s-2OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definir la función de decodificación\n",
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer({\n",
        "            \"encoder_inputs\": tokenized_input_sentence,\n",
        "            \"decoder_inputs\": tokenized_target_sentence\n",
        "        })\n",
        "        sampled_token_index = ops.convert_to_numpy(ops.argmax(predictions[0, i, :])).item(0)\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "# Métricas BLEU y ROUGE\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')  #Descargar tokenizer para BLEU\n",
        "\n",
        "def calculate_bleu(pairs, model, n_samples=100):\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    selected_pairs = random.sample(pairs, min(n_samples, len(pairs)))\n",
        "\n",
        "    for eng, spa in selected_pairs:\n",
        "        #Decodificar la traducción del modelo\n",
        "        translated = decode_sequence(eng)\n",
        "\n",
        "        #Procesar referencias e hipótesis\n",
        "        ref = spa.replace('[start]', '').replace('[end]', '').strip().split()\n",
        "        hyp = translated.replace('[start]', '').replace('[end]', '').strip().split()\n",
        "\n",
        "        references.append([ref])\n",
        "        hypotheses.append(hyp)\n",
        "\n",
        "    #Calcular BLEU con suavizado para evitar ceros\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
        "\n",
        "def calculate_rouge(pairs, model, n_samples=100):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_scores = []\n",
        "    rougeL_scores = []\n",
        "\n",
        "    selected_pairs = random.sample(pairs, min(n_samples, len(pairs)))\n",
        "\n",
        "    for eng, spa in selected_pairs:\n",
        "        translated = decode_sequence(eng)\n",
        "        ref = spa.replace('[start]', '').replace('[end]', '').strip()\n",
        "        hyp = translated.replace('[start]', '').replace('[end]', '').strip()\n",
        "\n",
        "        scores = scorer.score(ref, hyp)\n",
        "        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "        rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    return np.mean(rouge1_scores), np.mean(rougeL_scores)\n",
        "\n",
        "#Evaluación con las métricas\n",
        "print(\"\\nEvaluación de métricas de traducción:\")\n",
        "print(f\"BLEU score: {calculate_bleu(test_pairs, transformer, n_samples=300)}\")\n",
        "rouge1, rougeL = calculate_rouge(test_pairs, transformer, n_samples=300)\n",
        "print(f\"ROUGE-1: {rouge1:.4f}, ROUGE-L: {rougeL:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "yk691Eb_wSBW",
        "outputId": "58854706-35b3-41a5-94fe-c9085379e27b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluación de métricas de traducción:\n",
            "BLEU score: 0.1341316843687854\n",
            "ROUGE-1: 0.5559, ROUGE-L: 0.5478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Resultados 6.2 n-grams\n",
        "Solo modificamos *_eng_vectorization_* y agregamos _*ngrams=m_* para algun $m\\in\\mathbb{N}$.\n",
        "\n",
        "| Número de n-grams | Accuracy | Loss  | Val Accuracy | Val Loss |\n",
        "|-------------------|----------|-------|-------------|----------|\n",
        "| 2                 | 0.2392   | 2.0617 | 0.2311      | 2.2354   |\n",
        "| 4                 | 0.2361   | 2.1264 | 0.2285      | 2.2466   |\n",
        "| 6                 | 0.2354   | 2.1343 | 0.2301      | 2.2569   |\n",
        "\n",
        "Por lo tanto, tomamos $n-grams=2$ ya que es el que nos dio un mayor valor de validación en Accuracy.\n",
        "\n",
        "###Resultados 6.3 tasa de aprendizaje\n",
        "Continuamos con la tasa de aprendizaje $lr$, agregamos el optimizador ADAM para modificar su tasa de aprendizaje:\n",
        "\n",
        "| ADAM(lr)          | Accuracy | Loss  | Val Accuracy | Val Loss |\n",
        "|-------------------|----------|-------|-------------|----------|\n",
        "| lr=0.001          | 0.2289   | 1.7962 | 0.2208      | 2.0142   |\n",
        "| lr=0.005          | 0.2051   | 2.1636 | 0.2031      | 2.2623   |\n",
        "| lr=0.01           | 0.0502   | 5.9670 | 0.0499      | 5.8368   |\n",
        "\n",
        "Podemos notar como a medida de que aumentamos la tasa de aprendizaje del optimizador ADAM, la precisión disminuye y tambien aumenta la perdida del modelo. Esto último ocurre tanto en el entrenamiento como en la validación. Por lo tanto, se deben de balancear estas dos métricas como es el caso de la tasa de aprendizaje de $lr=0.001$ o $lr=0.005$ donde valores cercanos a esos valores darían mejores resultados tanto en la Accuracy como en la perdida, dependiendo que se quiera mejorar más.\n",
        "\n",
        "\n",
        "###6.4 Optimizador\n",
        "Cambiamos el optimizador cargando algun otro de keras y rn *_compile_* agregamos estos optimizadores los cuale son los siguientes:\n",
        "\n",
        "| Optimizador(lr)| Accuracy | Loss  | Val Accuracy | Val Loss |\n",
        "|-------------------|----------|-------|-------------|----------|\n",
        "| RMSprop(0.001)    |0.0503   | 5.8545 | 0.0504      | 5.7475   |\n",
        "| AdamW(0.001)         | 0.2481   | 1.3703 | 0.2293      | 1.8646   |\n",
        "| Lion           | 0.0590   | 5.6973 | 0.0599      | 5.5801   |\n",
        "\n",
        "Nuevamente, la variacion de ADAM, ADAMW fue el que obtuvo mejores resultados con errores de entrenamiento más bajos que el ADAM normal.\n",
        "\n",
        "###6.5 Métrica\n",
        "Al cambiar la métrica a *sparse categorical accuracy*, esta baja utilizando el ADAM(lr=0.001) de la siguiente manera:\n",
        "\n",
        "| Loss   | Sparse Categorical Accuracy | Val Loss | Val Sparse Categorical Accuracy |\n",
        "|--------|-----------------------------|----------|-----------------------------------|\n",
        "| 5.6287 | 0.0605                      | 5.5408   | 0.0601                           |\n",
        "\n",
        "Por lo cual no es conveniente cambiarla ya que aumenta la perdida.\n",
        "\n",
        "###6.6 y 6.7 Metricas BLUE (Bilingual Evaluation Understudy) y Rouge\n",
        "# Métricas de Evaluación para Traducción Automática/Texto Generado\n",
        "\n",
        "## **BLEU (Bilingual Evaluation Understudy)**\n",
        "- Mide la similitud entre texto generado y referencias humanas usando *coincidencia de n-gramas* (usualmente 1-4 gramas).\n",
        "- **Usos típicos**:\n",
        "  - Evaluación de traducción automática\n",
        "  - Sistemas de conversión texto-a-texto\n",
        "- **Limitaciones**:\n",
        "  - No considera significado semántico\n",
        "  - Pobre desempeño con idiomas flexibles\n",
        "  - Favorece traducciones literales\n",
        "\n",
        "## **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
        "-Mide la adecuación del contenido mediante *recuperación de unidades léxicas*.\n",
        "- **Usos típicos**:\n",
        "  - Evaluación de resúmenes\n",
        "  - Sistemas de generación de texto\n",
        "- **Limitaciones**:\n",
        "  - No evalúa coherencia\n",
        "  - Sensible a redundancias\n",
        "  - Ignora relaciones semánticas\n",
        "\n",
        "| Característica          | BLEU                      | ROUGE               |\n",
        "|-------------------------|---------------------------|---------------------|\n",
        "| Enfoque principal       | Precisión                 | Recall              |\n",
        "| Ideal para              | Traducción                | Resumen/Generación  |\n",
        "| Sensibilidad a longitud | Penaliza cortos           | Menos sensible      |\n",
        "| Unidad de análisis      | N-gramas exactos          | Subsecuencias       |\n",
        "| Tiempo de cálculo       | Rápido                    | Moderado            |\n",
        "| Métrica oficial en      | WMT (traducción)          | DUC (resumen)       |\n",
        "\n",
        "Pra interpretar los scores:\n",
        "- **BLEU**:\n",
        "  - mayor a 0.4: Excelente\n",
        "  - 0.3-0.4: Bueno\n",
        "  - menor a 0.2: Pobre\n",
        "- **ROUGE**:\n",
        "  - mayor a 0.5: Alta calidad\n",
        "  - 0.3-0.5: Aceptable\n",
        "  - menor a 0.2: Baja calidad\n",
        "\n",
        "Para nuestro ejemplo de 6 epocas con un optimizador Adam con *lr=0.001* y *ngrams=2* obtuvimos:\n",
        "- BLEU score:  0.1341316843687854.\n",
        "- ROUGE-L: 0.5478.\n",
        "\n",
        "Donde los modelos son pobres debido a las pocas epocas que se usaron."
      ],
      "metadata": {
        "id": "UBgU9mjkY3Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Guardar el modelo\n",
        "transformer.save(\"modelo_transformer.keras\")\n",
        "\n",
        "#Guardar los pesos\n",
        "transformer.save(\"transformer_pesos.h5\")\n",
        "\n",
        "# Guardar modelo con pretrained embedding\n",
        "transformer2.save(\"transformer_model_glove.keras\")\n",
        "\n",
        "transformer = keras.models.load_model(\n",
        "    \"transformer_model_rnd.keras\",\n",
        "    custom_objects={\n",
        "        \"encoder_inputs\": encoder_inputs,\n",
        "        \"decoder_inputs\": decoder_inputs,\n",
        "        \"TransformerEncoder\": TransformerEncoder,\n",
        "        \"TransformerDecoder\": TransformerDecoder,\n",
        "        \"PositionalEmbedding\": PositionalEmbedding,\n",
        "    },\n",
        ")\n",
        "\n",
        "transformer_glove = keras.models.load_model(\n",
        "    \"transformer_model_glove.keras\",\n",
        "    custom_objects={\n",
        "        \"encoder_inputs\": encoder_inputs,\n",
        "        \"decoder_inputs\": decoder_inputs,\n",
        "        \"TransformerEncoder\": TransformerEncoder,\n",
        "        \"TransformerDecoder\": TransformerDecoder,\n",
        "        \"PositionalEmbeddingGlove\": PositionalEmbeddingGlove,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "cewm-myRDlcy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2abd894e-4769-4255-a110-5190a36ca33e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoding test sentences"
      ],
      "metadata": {
        "id": "p9ySUm_egADq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spa_vocab = spa_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = spa_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            {\n",
        "                \"encoder_inputs\": tokenized_input_sentence,\n",
        "                \"decoder_inputs\": tokenized_target_sentence,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # ops.argmax(predictions[0, i, :]) is not a concrete value for jax here\n",
        "        sampled_token_index = ops.convert_to_numpy(\n",
        "            ops.argmax(predictions[0, i, :])\n",
        "        ).item(0)\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]"
      ],
      "metadata": {
        "id": "gioEuV2df8ex"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(30):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "\n",
        "print(input_sentence)\n",
        "print(translated)"
      ],
      "metadata": {
        "id": "aXTRqxPom1OD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "edd04367-065c-494a-e837-18021ad5d492"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like I'm not needed here.\n",
            "[start] parece que no tengo que no esté aquí [end]\n"
          ]
        }
      ]
    }
  ]
}