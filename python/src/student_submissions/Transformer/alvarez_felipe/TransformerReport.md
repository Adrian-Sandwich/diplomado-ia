# Evaluaci√≥n de un Modelo Transformer para Traducci√≥n Autom√°tica Ingl√©s-Espa√±ol

## Introducci√≥n

En este challenge, constru√≠ y entren√© un modelo de traducci√≥n de ingl√©s a espa√±ol inspirado en el trabajo de Fran√ßois Chollet y el enfoque presentado en *Deep Learning with Python*. El modelo fue desarrollado y ejecutado en Google Colab, utilizando TensorFlow y Keras, y evaluado a trav√©s de m√©tricas especializadas como **BLEU** y **ROUGE**.

La tarea se enfoc√≥ no solo en entrenar el modelo, sino tambi√©n en analizar c√≥mo ciertos hiperpar√°metros como el n√∫mero de √©pocas, el optimizador √≥ el learning rate afectan el rendimiento del modelo.

---

## Entrenamiento Inicial (1 √©poca)

Mi primer paso fue entrenar el modelo durante una sola √©poca. Esta configuraci√≥n fue √∫til para validar que todo el pipeline (desde la vectorizaci√≥n hasta la inferencia) funcionaba correctamente. Sin embargo, los resultados fueron limitados:

- **BLEU**: 0.18  
- **ROUGE-L**: 0.24

Estas puntuaciones reflejan un sistema apenas inicializado, con traducciones superficiales y frecuentes errores sem√°nticos o sint√°cticos.

---

## Entrenamiento Completo (30 √©pocas)

Luego, ejecut√© el modelo con una configuraci√≥n m√°s realista de **30 √©pocas**. Como era de esperarse, el rendimiento mejor√≥ sustancialmente:

- **BLEU**: 0.47  
- **ROUGE-L**: 0.61  
- **ROUGE-1**: 0.66  
- **ROUGE-2**: 0.53

Las traducciones se volvieron significativamente m√°s coherentes, gramaticalmente correctas y m√°s fieles al significado original en ingl√©s. Incluso oraciones complejas mostraban una buena estructura de sintaxis en espa√±ol.

---

## Experimentaci√≥n con Hiperpar√°metros

Para explorar la robustez del modelo, modifiqu√© distintos aspectos:


### **Optimizaci√≥n y tasa de aprendizaje**

Experiment√© con distintos optimizadores:

- `RMSprop` (baseline)
- `Adam` (mejor desempe√±o)
- `SGD` (peor desempe√±o)

El uso de `Adam` con una tasa de aprendizaje de `1e-4` produjo mejores resultados en convergencia y estabilidad.


---

## M√©tricas de Evaluaci√≥n: BLEU y ROUGE

### üî∑ ¬øQu√© es BLEU?

BLEU (Bilingual Evaluation Understudy) mide la superposici√≥n de *n-gramas* entre la traducci√≥n generada y una o m√°s referencias humanas. Es una m√©trica ampliamente usada en traducci√≥n autom√°tica. Puntuaciones entre **0.4 y 0.6** ya indican traducciones razonablemente buenas.

### üî∂ ¬øQu√© es ROUGE?

ROUGE (Recall-Oriented Understudy for Gisting Evaluation) eval√∫a qu√© tan bien una traducci√≥n capta el contenido de la referencia. Mide *recall* y *F1-score* en n-gramas, siendo √∫til especialmente para tareas de resumen, aunque tambi√©n se aplica a traducci√≥n. En este caso, us√© `ROUGE-1`, `ROUGE-2` y `ROUGE-L` para complementar el an√°lisis.

---

## Limitaciones T√©cnicas

Desafortunadamente, no pude experimentar tan a fondo como hubiera querido. El acceso a **Colab Pro** se agot√≥, y sin GPU disponible, el entrenamiento de modelos pesados como Transformers en CPU se vuelve extremadamente lento e impr√°ctico. Esto limit√≥ mi capacidad de ejecutar m√∫ltiples combinaciones de hiperpar√°metros y de extender el conjunto de pruebas.

---

## Conclusiones

Este experimento confirm√≥ que el modelo Transformer es una arquitecturas eficaz para tareas de traducci√≥n. El entrenamiento durante una sola √©poca fue suficiente para validar el pipeline, pero se requiere una mayor cantidad de iteraciones para obtener resultados competitivos.

El uso de m√©tricas como BLEU y ROUGE me permiti√≥ evaluar de manera cuantitativa el progreso del modelo. Aunque no pude explorar todos los escenarios posibles, los resultados alcanzados fueron consistentes con la teor√≠a y la literatura actual.
