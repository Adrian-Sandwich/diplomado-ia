{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3XsboZDjzd3j"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aONmV9Ch0fkA",
        "outputId": "27d6582c-6ffe-4f7f-e454-0ab587501a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available\n"
          ]
        }
      ],
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[-1], True)\n",
        "    print(\"GPU available\")\n",
        "else:\n",
        "    print(\"GPU not available, using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "c1qSYMGrzkWG"
      },
      "outputs": [],
      "source": [
        "def AlexNetScratch(input_shape=(224, 224, 3), num_classes=10):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(64, (11, 11), strides=4, padding='same', activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((3, 3), strides=2))\n",
        "    model.add(layers.Conv2D(192, (5, 5), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((3, 3), strides=2))\n",
        "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((3, 3), strides=2))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(4096, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WFKvGPfjB9MN"
      },
      "outputs": [],
      "source": [
        "def resize_images(images, new_size):\n",
        "    resized_images = tf.image.resize(images, new_size)\n",
        "    return resized_images\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Convertir los datos a tensores\n",
        "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
        "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
        "\n",
        "# Normalizar los datos\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Convertir las etiquetas a one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sJKLhulZV3mc"
      },
      "outputs": [],
      "source": [
        "# Create tf.data.Dataset objects\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "# Resize images on-the-fly during training\n",
        "train_dataset = train_dataset.map(lambda image, label: (resize_images(image, (224, 224)), label))\n",
        "test_dataset = test_dataset.map(lambda image, label: (resize_images(image, (224, 224)), label))\n",
        "\n",
        "# Batch the datasets\n",
        "batch_size = 64  # Adjust as needed\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "say7Xdh7zqKv",
        "outputId": "c310b044-15a6-4bcb-fd46-1c602ee2542d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 73ms/step - accuracy: 0.2192 - loss: 2.0836 - val_accuracy: 0.4638 - val_loss: 1.4619\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 63ms/step - accuracy: 0.4683 - loss: 1.4612 - val_accuracy: 0.5270 - val_loss: 1.3144\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 63ms/step - accuracy: 0.5403 - loss: 1.2864 - val_accuracy: 0.5623 - val_loss: 1.2134\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 61ms/step - accuracy: 0.5834 - loss: 1.1749 - val_accuracy: 0.6030 - val_loss: 1.1265\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - accuracy: 0.6122 - loss: 1.0916 - val_accuracy: 0.6107 - val_loss: 1.1110\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 64ms/step - accuracy: 0.6397 - loss: 1.0181 - val_accuracy: 0.6282 - val_loss: 1.0725\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 62ms/step - accuracy: 0.6634 - loss: 0.9530 - val_accuracy: 0.6262 - val_loss: 1.1164\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 64ms/step - accuracy: 0.6770 - loss: 0.9148 - val_accuracy: 0.6134 - val_loss: 1.1677\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 64ms/step - accuracy: 0.6932 - loss: 0.8698 - val_accuracy: 0.6276 - val_loss: 1.1566\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 63ms/step - accuracy: 0.7144 - loss: 0.8129 - val_accuracy: 0.6396 - val_loss: 1.1119\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - accuracy: 0.6457 - loss: 1.0886\n",
            "Test Loss: 1.1119, Accuracy: 0.64%\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "model = AlexNetScratch()\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_dataset, epochs=num_epochs, batch_size=batch_size, validation_data=test_dataset)\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f'Test Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voeGBuDnzr2f",
        "outputId": "ebe90ba4-e3ae-4f35-fb8f-aab71253a840"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model_path = \"alexnet_scratch_tensorflow.h5\"\n",
        "model.save(model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
