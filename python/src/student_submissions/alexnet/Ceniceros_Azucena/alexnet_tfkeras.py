# -*- coding: utf-8 -*-
"""AlexNet_TFKeras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1auE7eyIGgk41UIJN2kp3PRwjLtcQXtBX

# **AlexNet from Scratch in TF**

Instalaci√≥n de librerias/herramientas necesarias
"""

!pip install tensorflow

from google.colab import drive
drive.mount('/content/drive')

import os
import pickle
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models

"""  Cargar el archivo CIFAR-10 y devolver el diccionario de datos"""

def unpickle(file):
    with open(file, 'rb') as fo:
        data_dict = pickle.load(fo, encoding='bytes')
    return data_dict

data_dir = '/content/drive/My Drive/cifar-10-batches-py'

"""Cargar los datos de CIFAR-10 y procesarlos"""

def load_cifar10_data(data_dir):
    train_data = []
    train_labels = []

    # Cargar los batches
    for i in range(1, 6):
        batch = unpickle(os.path.join(data_dir, f"data_batch_{i}"))
        train_data.append(batch[b'data'])
        train_labels += batch[b'labels']

   # Convertir los datos en un array y hacer reshape
    train_data = np.concatenate(train_data)
    train_data = train_data.reshape((train_data.shape[0], 32, 32, 3))

    # Cargar datos de prueba y hacer reshape
    test_batch = unpickle(os.path.join(data_dir, "test_batch"))
    test_data = test_batch[b'data']
    test_labels = test_batch[b'labels']
    test_data = test_data.reshape((test_data.shape[0], 32, 32, 3))

    return (train_data, np.array(train_labels)), (test_data, np.array(test_labels))

(train_data, train_labels), (test_data, test_labels) = load_cifar10_data(data_dir)

# Normalizar los valores al rango de 0 a 1
train_data = train_data.astype('float32') / 255.0
test_data = test_data.astype('float32') / 255.0

# One-hot encoding
train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

"""Arquitectura del modelo"""

def build_alexnet(input_shape=(32, 32, 3), num_classes=10):
    model = models.Sequential()

    # Feature extraction
    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape))
    model.add(layers.MaxPooling2D((2, 2), strides=2))

    model.add(layers.Conv2D(192, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2), strides=2))

    model.add(layers.Conv2D(384, (3, 3), activation='relu', padding='same'))

    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))

    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))
    model.add(layers.MaxPooling2D((2, 2), strides=2))

    # Flatten & fully connected layers
    model.add(layers.Flatten())
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(4096, activation='relu'))
    model.add(layers.Dropout(0.5))
    model.add(layers.Dense(4096, activation='relu'))
    model.add(layers.Dense(num_classes, activation='softmax'))  # Output layer

    return model

# Construir el modelo
model = build_alexnet()
model.summary()

"""Compilar el modelo"""

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

"""Entrenar el modelo"""

history = model.fit(train_data, train_labels,
                    epochs=20,
                    batch_size=64,
                    validation_data=(test_data, test_labels))

"""Evaluacion del modelo"""

test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)
print(f"Accuracy: {test_acc:.4f} %")